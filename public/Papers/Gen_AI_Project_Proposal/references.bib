@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished{\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@misc{hoffmann2022trainingcomputeoptimallargelanguage,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.15556}, 
}

@article{merity2017pointer,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2017}
}

@article{marcus1993building,
  title={Building a Large Annotated Corpus of English: The Penn Treebank},
  author={Marcus, Mitchell P. and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  journal={Computational Linguistics},
  volume={19},
  number={2},
  pages={313--330},
  year={1993}
}



@misc{karbevski2025keyvalueweightsprobably,
      title={Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers}, 
      author={Marko Karbevski and Antonij Mijoski},
      year={2025},
      eprint={2510.23912},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2510.23912}, 
}

@misc{peebles2023scalable,
      title={Scalable Diffusion Models with Transformers}, 
      author={William Peebles and Saining Xie},
      year={2023},
      eprint={2212.09748},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{he2023localized,
      title={Localized Text-to-Image Generation for Free via Cross Attention Control}, 
      author={Yutong He and Ruslan Salakhutdinov and J. Zico Kolter},
      year={2023},
      eprint={2306.14636},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{song2021scorebased,
      title={Score-Based Generative Modeling through Stochastic Differential Equations}, 
      author={Yang Song and Jascha Sohl-Dickstein and Diederik P. Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
      year={2021},
      eprint={2011.13456},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{he2024simplifyingtransformerblocks,
      title={Simplifying Transformer Blocks}, 
      author={Bobby He and Thomas Hofmann},
      year={2024},
      eprint={2311.01906},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.01906}, 
}

@inproceedings{Bermeitinger_2024,
   title={Reducing the Transformer Architecture to a Minimum},
   url={http://dx.doi.org/10.5220/0012891000003838},
   DOI={10.5220/0012891000003838},
   booktitle={Proceedings of the 16th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management},
   publisher={SCITEPRESS - Science and Technology Publications},
   author={Bermeitinger, Bernhard and Hrycej, Tomas and Pavone, Massimo and Kath, Julianus and Handschuh, Siegfried},
   year={2024},
   pages={234–241} }

@misc{zhussip2025shareattentiontransformerweight,
      title={Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning}, 
      author={Magauiya Zhussip and Dmitriy Shopkhoev and Ammar Ali and Stamatios Lefkimmiatis},
      year={2025},
      eprint={2508.04581},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2508.04581}, 
}

@misc{wang2020linformerselfattentionlinearcomplexity,
      title={Linformer: Self-Attention with Linear Complexity}, 
      author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
      year={2020},
      eprint={2006.04768},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.04768}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{ainslie2023gqatraininggeneralizedmultiquery,
      title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints}, 
      author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebrón and Sumit Sanghai},
      year={2023},
      eprint={2305.13245},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13245}, 
}

@misc{shazeer2019fasttransformerdecodingwritehead,
      title={Fast Transformer Decoding: One Write-Head is All You Need}, 
      author={Noam Shazeer},
      year={2019},
      eprint={1911.02150},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1911.02150}, 
}

@misc{ainslie2023gqatraininggeneralizedmultiquery,
      title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints}, 
      author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebrón and Sumit Sanghai},
      year={2023},
      eprint={2305.13245},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13245}, 
}

@misc{bhojanapalli2020lowrankbottleneckmultiheadattention,
      title={Low-Rank Bottleneck in Multi-head Attention Models}, 
      author={Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},
      year={2020},
      eprint={2002.07028},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.07028}, 
}

@misc{cordonnier2021multiheadattentioncollaborateinstead,
      title={Multi-Head Attention: Collaborate Instead of Concatenate}, 
      author={Jean-Baptiste Cordonnier and Andreas Loukas and Martin Jaggi},
      year={2021},
      eprint={2006.16362},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.16362}, 
}

@misc{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@misc{aghajanyan2020intrinsicdimensionalityexplainseffectiveness,
      title={Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning}, 
      author={Armen Aghajanyan and Luke Zettlemoyer and Sonal Gupta},
      year={2020},
      eprint={2012.13255},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2012.13255}, 
}

@misc{amsel2024benefitsrankattentionlayers,
      title={On the Benefits of Rank in Attention Layers}, 
      author={Noah Amsel and Gilad Yehudai and Joan Bruna},
      year={2024},
      eprint={2407.16153},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.16153}, 
}

@misc{allenzhu2019convergencetheorydeeplearning,
      title={A Convergence Theory for Deep Learning via Over-Parameterization}, 
      author={Zeyuan Allen-Zhu and Yuanzhi Li and Zhao Song},
      year={2019},
      eprint={1811.03962},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1811.03962}, 
}

@misc{balzano2025overviewlowrankstructurestraining,
      title={An Overview of Low-Rank Structures in the Training and Adaptation of Large Models}, 
      author={Laura Balzano and Tianjiao Ding and Benjamin D. Haeffele and Soo Min Kwon and Qing Qu and Peng Wang and Zhangyang Wang and Can Yaras},
      year={2025},
      eprint={2503.19859},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.19859}, 
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{dao2022flashattentionfastmemoryefficientexact,
      title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, 
      author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
      year={2022},
      eprint={2205.14135},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.14135}, 
}

@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Gokaslan2019OpenWeb,
    title={OpenWebText Corpus},
    author={Gokaslan, Aaron and Cohen, Vanya and Pavlick, Ellie and Tellex, Stefanie},
    howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}},
    year={2019}
}