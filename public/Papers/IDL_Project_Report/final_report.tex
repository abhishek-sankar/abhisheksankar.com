\documentclass[10pt]{article}
\usepackage{ragged2e} % Enables text justification
% 2. **Apply global text justification:**
\justifying % Makes all text justified by default
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} % Wider left/right margins, keep top margin small
\usepackage{ragged2e}

\usepackage{booktabs}
\justifying % Justify all text throughout the document

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath, amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
\setlist{noitemsep, topsep=2pt, leftmargin=5pt}
\setlength{\abovedisplayskip}{7pt}
\setlength{\belowdisplayskip}{7pt}
\setlength{\parindent}{0pt} % No indentation for all paragraphs

\usepackage{natbib}
\usepackage{sectsty}
\sectionfont{\large}
\subsectionfont{\normalsize}
\usepackage{titlesec}
\titlespacing{\subsection}{0pt}{*0.4}{*0.4}
\titlespacing{\subsubsection}{0pt}{*0.3}{*0.4}

\title{ProjeXion: Precision 3-D Modeling from 2-D Inputs}
\author{\small Team 18: ProjeXion \\ 
    \small Nikita Chaudhari, Maitri Gada, Abhishek Sankar, Santiago Arámbulo \\
    \small \{nikitac, mbgada, asankar2, sarambul\}@cs.cmu.edu}
\date{April 2025}

\begin{document}

\maketitle

\begin{abstract}
% This report presents our work on precision 3D reconstruction from 2D inputs using deep learning approaches. We implement and extend the MVSNet architecture for accurate depth estimation from multi-view images using the BlendedMVS dataset. Our approach combines convolutional neural networks as feature extractors, self-attention as an aggregation procedure, differentiable homography warping for cost volume construction, and 3D CNN regularization depth prediction refinement. Our implementation addresses key challenges in 3D reconstruction including occlusions, scale variations, lighting conditions, and multi-view fusion. The results demonstrate promising performance in reconstructing detailed 3D depth maps from 2D images, with potential applications in AR/VR, robotics, digital twins, and architecture.

3D reconstruction from 2D images is a central problem in computer vision, with critical applications in robotics, AR/VR, digital twins, and autonomous systems. Despite recent progress, challenges such as occlusion, scale ambiguity, lighting variation, and multiview fusion continue to limit reconstruction accuracy. Traditional photogrammetry pipelines are often fragile and computationally expensive, frequently requiring extensive manual calibration. In this work, we implement and extend MVSNet, a deep learning-based multi-view stereo architecture for depth estimation. Our approach integrates convolutional neural networks as feature extractors, differentiable homography warping for constructing a cost volume, and 3D CNN-based regularization for depth regression. We further introduce ProjeXion, an architectural extension that incorporates a self-attention-based fusion module to better aggregate information across views. Training on the BlendedMVS dataset, we observe a significant divergence between the distribution of depth values for different objects, which generates large errors for some views. This leads us to adopt a Cauchy Loss instead of the traditional MSE Loss, to increase the robustness to large errors. While our baseline MVSNet implementation captures fine object-level details and demonstrates high-fidelity reconstructions, ProjeXion highlights the difficulty of improving strong baselines through naive architectural extensions. Our ablation studies (e.g., removing cost volume regularization) reveal how tightly coupled each component is to final performance, emphasizing the importance of thoughtful, data-aware design. Rather than delivering immediate performance gains, our work surfaces architectural and optimization bottlenecks—pointing to the need for more principled fusion strategies and robust, generalizable training objectives in future MVS systems.

\end{abstract}

\section{Introduction}
Reconstructing accurate 3D models from 2D images is a longstanding challenge in computer vision, with wide-ranging applications in robotics, AR/VR, digital twins, and architecture. Robust 3D perception enables autonomous navigation, immersive rendering, and virtual inspection capabilities that are particularly valuable in safety-critical and accessibility-sensitive domains.

Despite its importance, 3D reconstruction remains difficult due to occlusions, scale ambiguity, lighting variation, and noise in camera pose estimation. Classical Multi-View Stereo (MVS) and photogrammetry pipelines, such as COLMAP \cite{COLMAP}, often suffer from brittle performance, high computational cost, and the need for manual parameter tuning, limitations that hinder their deployment in real-world settings.

Recent advances in deep learning present promising alternatives. Neural architectures like MVSNet \cite{MVSNet} combine convolutional feature extractors with differentiable homography warping to construct a 3D cost volume, which is then regularized by a 3D CNN to predict dense per-view depth maps. These end-to-end learned pipelines offer improved accuracy and faster inference compared to traditional methods.

In this project, we reproduce the MVSNet architecture and propose \textbf{ProjeXion}, a novel extension that incorporates a self-attention-based recurrent fusion module to enhance multi-view feature aggregation. Our goals are twofold: (1) \textit{Qualitative}—to reconstruct detailed geometry with high visual fidelity, and (2) \textit{Quantitative}—to evaluate depth accuracy using standard metrics. We train on the BlendedMVS dataset to promote generalization across diverse, realistic scenes, and analyze both convergence behavior and architectural robustness. While the baseline MVSNet achieves strong performance, our results highlight the complexity of extending high-performing MVS systems and offer insights into the limits of current fusion strategies.
% \subsection{Overview and Context}
% % 3D reconstruction from 2D images represents a fundamental challenge in computer vision with significant applications across multiple domains. The ability to accurately predict depth and generate 3D models from 2D inputs is crucial for scene understanding, localization, and object interaction in various fields including robotics, augmented and virtual reality, architecture, and many other applications requiring spatial awareness.

% Reconstructing accurate 3D odels from 2D images is a long-standing challenge in computer vision and photogrammetry. Success in this domain unlocks capabilities across a wide range of technologies: autonomous robots rely on 3D perception to navigate without collision; AR/VR systems use detailed depth maps to enhance realism and reduce visual jitter; digital twins and architectural systems depend on high-fidelity 3D reconstructions for virtual inspection and modeling of real-world environments. The significance of this task is further amplified in safety- or accessibility-sensitive applications, for example, enabling consistent spatial cues in neurodivergent friendly spaces.
% Accurate 3D reconstruction from images remains difficult - Multi-view fusion, the process of aggregating information from multiple images taken at varying viewpoints introduces substantial challenges.


\subsection{Motivation}
3D model reconstruction from images involves multiple unresolved technical challenges, including occlusion handling, scale ambiguity, lighting variability, and effective multi-view fusion. Existing approaches often fall short: traditional photogrammetry pipelines demand extensive manual tuning, AR systems exhibit perceptual jitter, and autonomous agents struggle with depth perception in scenes with reflective or transparent surfaces.

Moreover, improved reconstruction fidelity has societal relevance - for instance, in designing neurodivergent-friendly environments that require consistent and accurate spatial feedback. Advancing learning-based 3D modeling techniques can thus enable more inclusive and responsive spatial computing applications.


\subsection{Objective}
% Our primary objective is to reproduce and extend the MVSNet pipeline \cite{MVSNet} for depth estimation from multi-view images. We use MVSNet as our baseline and introduce our own experiments and improvements to enhance depth estimation accuracy and efficiency. We specifically focus on improving the pipeline's performance on the BlendedMVS dataset, which offers more diverse and realistic scenarios compared to lab-controlled datasets, such as DTU \cite{dtu}.
Our primary objective is to reproduce and extend the MVSNet pipeline \cite{MVSNet} for depth estimation from multi-view images. We use MVSNet as a baseline and implement architectural extensions aimed at improving depth estimation accuracy and training stability. Specifically, we evaluate our system on the BlendedMVS dataset, chosen for its large scale and diversity, which better reflects real-world variation compared to controlled datasets such as DTU \cite{dtu}.
%--------------------------------------------------------------------
% Literature Review
%--------------------------------------------------------------------

\section{Literature Review}
Multi-View Stereo (MVS) has undergone significant evolution, progressing from classical geometric pipelines to modern deep learning frameworks. This section groups related prior work into five thematic categories and situates our contribution within this landscape.

\subssection{Classical MVS Pipelines}

Traditional multi-view stereo (MVS) pipelines rely on multi-view geometry and photometric consistency without learning. A representative system is COLMAP\cite{COLMAP}, which first uses structure-from-motion to estimate camera poses, then performs dense stereo matching to recover depth maps for each image, followed by fusion into a 3D point cloud or mesh. COLMAP’s MVS module uses a patch-based stereo algorithm (PatchMatch) to iteratively refine per-pixel depth by searching for photo-consistent matches across views. Hand-crafted metrics (e.g. normalized cross-correlation) with robust losses (e.g. truncated or Cauchy loss) are used to evaluate patch similarity, reducing the impact of outliers. This classical approach excels in textured regions under Lambertian assumptions but struggles with low-texture, reflective surfaces and requires many input images for completeness. Nonetheless, geometry-based methods like COLMAP remain strong baselines, often achieving high accuracy on benchmarks without any training data. For example, COLMAP reconstructions on the DTU dataset achieve accuracy/completeness on par with early learned methods, and it scored competitively on the Tanks and Temples benchmark (2017) before deep learning methods emerged. The classical paradigm informed later learning-based MVS by providing a pipeline (feature matching → depth estimation → fusion) and techniques like per-view depth map computation and visibility filtering that many deep networks emulate or integrate.

\subsection{Early Learning-Based MVS}
Initial attempts to apply deep learning to MVS replaced parts of the classical pipeline with neural networks. SurfaceNet \cite{SurfaceNet} was one of the first end-to-end learned MVS framework. It encodes multiple images and known cameras into a voxel-based 3D volume and uses a fully 3D CNN to classify each voxel as being on the surface or not. This volumetric approach directly learns photometric consistency and surface smoothness from data, rather than using separate hand-engineered steps. SurfaceNet demonstrated that a CNN could jointly infer a 3D surface from multiple images, yielding completeness comparable to classical methods on DTU. However, its memory requirements limited the volume resolution (e.g. $32^3$ voxels) and thus the detail of reconstructions. Other early works took different approaches: Huang et al. (2018) proposed DeepMVS \cite{deepmvs}, which introduced plane-sweep cost volumes for each view and a 2D U-Net to regress depth maps. The plane-sweep volume stores multiple fronto-parallel warps of source images w.r.t. a reference view (at discrete depth hypotheses), encoding photometric evidence similar to classical plane-sweeping stereo. A deep network then processes this volume to predict the depth. These early deep MVS methods proved that learned features can improve matching in challenging conditions (textureless or reflective areas) and that multi-view aggregation can be learned, but they were often slower or less scalable than classical MVS. SurfaceNet’s heavy 3D convolution made it computationally expensive, while DeepMVS required sequential processing of plane-sweep volumes. They also often trained on smaller datasets (e.g. DTU \cite{dtu}, a scanned object dataset) and had limited generalization to large scenes. Nonetheless, they inspired the use of differentiable homography warping (plane sweeping) and the idea of end-to-end depth map inferencethat became central in subsequent works.

\subsection{Cost Volume-Based Depth Estimation}
A breakthrough in learnable MVS came with methods that construct 3D cost volumes and apply deep regularization, enabling high-quality depth estimation from multiple views. MVDepthNet \cite{MVDepthNet} introduced a real-time pipeline using plane-sweep volumes. and a lightweight 2D CNN, showing the feasibility of fast multi-view depth estimation. The definitive approach, however, was MVSNet \cite{MVSNet}, which established the core architecture adopted by many later works. MVSNet extracts deep features from each input image and warps them into a stacked cost volume via homographies, aligned with a reference camera frustum. A 3D CNN then regularizes the cost volume (essentially performing spatial–depth filtering) and regresses a probability distribution over depth for each pixel. Finally, soft argmin is used to obtain the depth map, which can be further refined with a 2D CNN using the reference image features  \cite{Wang2024Survey}. This end-to-end pipeline achieved significantly lower error on DTU (e.g. overall ~0.462 mm) than prior methods and higher completeness on Tanks and Temples (mean score ~43.5\% on the intermediate set)\cite{MVSNet}. Key technical ideas of MVSNet include the differentiable homography warping (enabling backpropagation through the cost volume construction) and a variance-based cost metric to aggregate multi-view features efficiently\cite{MVSNet}. MVSNet’s design decoupled the per-view feature extraction (2D CNN) from the multi-view fusion (3D CNN on cost volume), inheriting the best of both worlds: learned feature matching and global regularization across views. This quickly became the foundation for numerous extensions. Notably, MVSNet and similar depth map-based methods allow processing of each reference view independently (an “offline” MVS approach\cite{Wang2024Survey}), which is memory-intensive but yields high-quality depth maps that can be fused to a point cloud. The student’s project is directly based on MVSNet’s architecture, using its cost volume construction and depth prediction as a starting point. However, vanilla MVSNet uses simple mean/variance feature aggregation and a fixed 3D convolution for all views, which the student aims to improve upon.

\subsection{Architectural Improvements Beyond MVSNet}
Subsequent work builds on MVSNet’s cost-volume formulation with architectural innovations aimed at reducing memory and improving accuracy. DPSNet \cite{DPSNet} reintroduces the plane-sweep stereo principle with a learnable framework and context-aware cost aggregation. R-MVSNet \cite{RMVSNet} and CasMVSNet \cite{CasMVSNet} introduce recurrent and cascade-based refinement, enabling coarse-to-fine prediction over reduced memory footprints. These approaches emphasize improved fusion and progressive refinement, though they inherit MVSNet’s reliance on fixed sampling strategies.

Atlas \cite{Atlas} takes a different direction: instead of estimating depth maps, it regresses a 3D volumetric representation (TSDF) directly from input images. This end-to-end model fuses feature maps via geometric “splatting” into a voxel grid and optimizes the full 3D volume jointly. It achieved higher completeness on datasets like ScanNet compared to pipelines combining MVSNet with external TSDF fusion. However, Atlas sacrifices efficiency, being memory-bound and limited in resolution. Our work instead focuses on enhancing the depth-map-based paradigm for scalability and efficiency, particularly suited for high-resolution scenes.

\subsection{Single-View 2D-to-3D Reconstruction}
Parallel to multi-view approaches, recent single-view methods focus on reconstructing 3D from limited inputs using strong shape priors. Splatter Image \cite{Szymanowicz_2024_CVPR} uses Gaussian splatting to regress point-based 3D representations at real-time speeds. Part123 \cite{liu2024part123} adopts a semantic part-based decomposition to improve structural consistency, while 3DPX \cite{Li_3DPX_MICCAI2024} reconstructs anatomical volumes from dental X-rays using a hybrid CNN–MLP pipeline. These methods are domain- or object-specific, often trading generalization or accuracy for speed. Our focus remains on general-purpose multi-view reconstruction, where accuracy can be improved through geometric constraints across multiple views rather than prior-based assumptions.

\begin{table}[htbp]
\centering
\caption{Summary of representative MVS methods.}
\small
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Method} & \textbf{Architecture Type} & \textbf{Training Data} \\
\midrule
COLMAP (2016)        & Geometric patch-match; depth fusion         & None (not learned)            \\
SurfaceNet (2017)    & 3D CNN on voxel grid (volumetric)           & DTU (objects)                 \\
MVDepthNet (2018)    & Plane-sweep + 2D U-Net (online)             & Sun3D, Scenes11               \\
MVSNet (2018)        & Cost volume + 3D CNN                        & DTU (train), no fine-tune     \\
R-MVSNet (2019)      & Recurrent GRU regularization                & DTU + Tanks fine-tune         \\
CasMVSNet (2020)     & Cascade coarse-to-fine volumes              & DTU + Tanks fine-tune         \\
TransMVSNet (2022)   & CNN + Transformer encoder                   & BlendedMVS + DTU              \\
UniMVSNet (2022)     & Hybrid (classification + regression)        & BlendedMVS                    \\
Atlas (2020)         & 2D CNN + back-proj + 3D CNN (TSDF)          & ScanNet (RGB-D)               \\
SplatterImage (2024) & 2D CNN to Gaussian splats                   & Synthetic (ShapeNet, etc.)    \\
Part123 (2024)       & Diffusion + SAM + NeRF rendering            & Diffusion/SAM (per object)    \\
\bottomrule
\end{tabular}
\label{tab:mvs-methods-simple}
\end{table}



\subsection{Summary and Positioning}
Overall, the field has progressed from geometric pipelines to learned systems that embed camera geometry within differentiable cost volumes and leverage 3D CNNs for regularization. MVSNet stands out for its balance between generalization, accuracy, and efficiency, and forms the basis for our work. In this project, we propose \textbf{ProjeXion}, an extension of MVSNet that introduces a self-attention-based recurrent fusion module for improved multi-view feature aggregation. This is inspired by recent success of attention mechanisms in vision models and recurrent refinement in MVS. We also adopt training stabilization techniques such as gradient accumulation to improve convergence, especially on complex datasets like BlendedMVS. Our method remains depth-map based for computational efficiency, but aims to improve fusion quality and generalization—key challenges identified in the literature.

%--------------------------------------------------------------------
% \subsection{Literature Review}
% %--------------------------------------------------------------------

% Multi-View Stereo (MVS) has evolved through several stages, from classical geometry–based methods to modern deep-learning approaches.  Below we briefly review representative works and outline how they inform our project.

% %--------------------------------------------------------------------
% \subsubsection{SfM/MVS Traditional Pipeline}

% \paragraph{COLMAP.}  
% COLMAP \cite{COLMAP} combines structure-from-motion and multi-view stereo, providing robust camera-pose estimation, bundle adjustment, and dense patch-based fusion.  Hand-crafted photometric costs and multiple global optimisation steps, however, limit its speed and robustness under wide baselines or low-texture scenes.

% %--------------------------------------------------------------------
% \subsubsection{Early Learning-based MVS}

% \paragraph{SurfaceNet.}  
% SurfaceNet \cite{SurfaceNet} was the first end-to-end 3-D CNN for multi-view stereopsis, discretising the scene into a voxel grid and predicting occupancy directly.  However, memory requirements grow cubically with resolution, restricting its practical application in common day scenarios.

% %--------------------------------------------------------------------
% \subsubsection{Deep Cost-Volume Methods}

% The next generation—including MVDepthNet \cite{MVDepthNet} and MVSNet \cite{MVSNet}—shifted to per-view depth estimation.

% \begin{itemize}
%     \item \textbf{MVDepthNet} encodes multi-view cues in a differentiable cost volume and regresses depth with an encoder–decoder CNN, achieving real-time inference for a variable number of inputs.
%     \item \textbf{MVSNet} introduces differentiable homography warping to build a 3-D cost volume; a 3-D U-Net regularises this volume and a soft-argmin yields sub-pixel depths.  Without retraining, it surpassed the Tanks \& Temples benchmark, showing strong generalisation.
% \end{itemize}

% %--------------------------------------------------------------------
% \subsubsection{Further Improvements in Learned MVS}

% Building on MVSNet, DPSNet \cite{DPSNet} integrates plane-sweep stereo with context-aware aggregation, while cascade and recurrent variants (e.g.\ CasMVSNet, R-MVSNet) reduce memory via coarse-to-fine refinement.  
% Meanwhile, Atlas \cite{Atlas} regresses a volumetric TSDF directly, jointly optimising depth and fusion at the cost of increased memory.

% %--------------------------------------------------------------------
% \subsubsection{Single-View 2-D \texorpdfstring{$\rightarrow$}{→} 3-D Reconstruction}

% Single-view methods lean on strong shape priors:

% \begin{itemize}
%     \item Splatter Image \cite{Szymanowicz_2024_CVPR} achieves ${\sim}38$ FPS by predicting Gaussian splats from a single image.  
%     \item Part123 \cite{liu2024part123} reconstructs objects part-by-part for improved structural consistency.  
%     \item 3DPX \cite{Li_3DPX_MICCAI2024} tackles dental imagery with a hybrid MLP–CNN pipeline.  
% \end{itemize}

% These later works emphasize speed or domain knowledge, whereas our focus is on multiview settings that exploit multiple observations for higher accuracy.

% %--------------------------------------------------------------------

\subsection{Background and Research Gap}
Multi-View Stereo (MVS) has advanced from classical geometry-based methods to deep learning frameworks that embed camera priors directly into neural architectures. Traditional pipelines like COLMAP~\cite{COLMAP} offer strong geometric consistency but are brittle in real-world scenes and computationally intensive. Learning-based systems such as SurfaceNet~\cite{SurfaceNet} and MVSNet~\cite{MVSNet} improve efficiency and accuracy through cost volume construction, feature warping, and 3D CNN regularization. The broader evolution of these methods—from geometry-driven designs to end-to-end neural architectures—is summarized in Table~\ref{tab:mvs-evolution}.


\begin{table}[h]
\centering
\begin{tabular}{cll}
\toprule
Year & Method & Key Trait \\
\midrule
2016 & COLMAP & Classical SfM \& dense fusion \\
2018 & MVDepthNet & First deep stereo MVS \\
2019 & DPSNet & Learned plane sweep stereo \\
2020 & Atlas & TSDF prediction, no explicit fusion \\
2025 & ProjeXion (Ours) & RNN-based fusion + fast batched metrics \\
\bottomrule
\end{tabular}
\caption{Evolution of Multi-View Stereo (MVS) Approaches}
\label{tab:mvs-evolution}
\end{table}

Despite these gains, current MVS systems face limitations in fusion quality and generalization. Most use fixed aggregation strategies—like mean or variance—to combine multi-view features, ignoring occlusions, view quality, or oblique angles. This uniform treatment degrades performance in scenes with diverse geometry or lighting. Furthermore, many models are trained on controlled datasets (e.g., DTU\cite{dtu}), limiting their robustness in more complex environments.

A related bottleneck is the rigidity of cost volume regularization. Typically, a fixed 3D CNN is used regardless of scene complexity or viewpoint diversity, limiting adaptability. Combined with the use of MSE-based loss functions, this leads to poor handling of high-error regions and unstable convergence during training.

ProjeXion addresses these gaps with two key contributions:

\begin{itemize}
\setlength\itemindent{3em}
    \item Cross-View Attention Fusion: A learnable self-attention module replaces variance-based fusion to adaptively weight source views at each depth-location.
    \item Robust Training Objective:A Cauchy loss improves resilience to outliers and enhances convergence on high-variance scenes like those in BlendedMVS.
\end{itemize}

Our approach aims to improve depth precision and structural fidelity while preserving the modular efficiency of depth-map-based MVS.

% In 2024, several significant advancements have emerged in the field, including:
% \begin{itemize}
%     \item Splatter Image \cite{Szymanowicz_2024_CVPR} for ultra-fast single-view 3D reconstruction 
%     \item Part123 \cite{liu2024part123} introducing part-aware 3D reconstruction from single-view images
%     \item 3DPX \cite{Li_3DPX_MICCAI2024} showing progressive 2D-to-3D oral image reconstruction with hybrid MLP-CNN networks
% \end{itemize}

% These recent works demonstrate the field's continued progress toward more accurate, faster, and specialized 3D reconstruction methods.
%--------------------------------------------------------------------
\section{Methodology}
%--------------------------------------------------------------------

\subsection{Pipeline Overview}
Let $\mathcal{I}=\{I_i\}_{i=0}^{T}$ be $T + 1$ calibrated views, with $I_0$ the reference view.  
Our network predicts a depth map $D_0$ through six stages, shown in
Fig.~\ref{fig:pipeline} and summarised below.

\begin{enumerate}
    \item \textbf{Feature Extraction (ImageEncoder).}  
          An 8-layer CNN (strides 1,1,2,1,1,2,1,1) produces $32$-channel feature maps
          $F_i\in\mathbb{R}^{32\times H/4\times W/4}$ for every view.
    \item \textbf{Plane-Sweep Warping (Homography).}  
          For a set of $D$ depth planes $\{d_k\}$ we compute homographies
          $H_{i\leftarrow0}(d_k)$ and warp $F_i$ to the reference view coordinate system, yielding a
          6-D tensor
          $\mathcal{V}\!\in\!\mathbb{R}^{T\times D\times32\times H/4\times W/4}$.
    \item \textbf{Cross-View Self-Attention.}  
          A \emph{SelfAttentionLayer} replaces the classic variance operator:
          for each pixel‐depth cell it attends over the $N$ views and returns an
          attended cost cube
          $C\in\mathbb{R}^{D\times32\times H/4\times W/4}$ for the reference
          view only.  A learnable scale $\gamma$ blends the attended response
          with the original reference features.
    \item \textbf{3-D Cost Regularisation (U-Net).}  
          A 3-D U-Net (\texttt{CostRegularizer}) with encoder
          $\{8,16,32,64\}$ channels and symmetric decoder reduces $C$ to a
          single-channel logit volume
          $\tilde{C}\in\mathbb{R}^{1\times D\times H/4\times W/4}$.
    \item \textbf{Depth Regression (SoftArgmin).}  
          Softmax along $d_k$ converts logits to probabilities
          $P(d_k|u,v)$; the expected depth
          $\hat{D}_0(u,v)=\sum_k d_k P(d_k|u,v)$ forms the coarse map.
    \item \textbf{Edge Refinement.}  
          A lightweight 2-D CNN (\texttt{Refine}) takes $(I_0,\hat{D}_0)$ and
          outputs the final high-resolution depth $D_0$.
\end{enumerate}

%--------------------------------------------------------------------
\subsection{Loss Function}
We adopt a \emph{masked Cauchy loss}
\[
\mathcal{L}=\frac{\smash{\sum_{(u,v)\in\Omega}}
        \tfrac{c^2}{2}\,
        \log\!\Bigl(1+\tfrac{(D_0-\!D_0^{\text{gt}})^2}{c^2}\Bigr)}
      {\sum_{(u,v)\in\Omega}1},
\]
where $\Omega$ denotes pixels with valid ground truth and $c$ is a tunable
robustness constant.  This down-weights extreme outliers compared to MSE.

%--------------------------------------------------------------------
\subsection{Implementation Notes}
\begin{itemize}
    % \item \textbf{Depth sampling:} $D{=}192$ inverse planes in $[d_{\min},d_{\max}]$,           stored as a static tensor to avoid re-allocation.
    \item \textbf{Context pairing:} Each object was several views available. For view pair, a similarity score was calculated. Then, for each view, a list of the closest 10 other views was built. This process was performed by the authors of \cite{MVSNet} and taken as given. When extracting context views for training, the first $T$ views from this ordered list were selected. 
    \item \textbf{Batching:} For each image, $T=5$ context views were extracted. If there were less thant $T$ views available, the missing values were padded with zeros. Batch size of $N=32$ used. All inputs shaped $(N,1+T,C,H,W)$ with $T{=}5$ source views per reference during training.
    \item \textbf{Training:} AdamW optimizer with learning rate of $5\times10^{-4}$, $\beta_1 = 0.9$ and $\beta_2=0.999$, and weight decay of 0.01. Trained on 70\% of the training dataset for 8 epochs using a constant scheduler.
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{IDL Project Diagram.jpg}
    \caption{Proposed pipeline: Based on MVSNet \cite{MVSNet} but applying self-attention cost aggregation instead of variance.}
    \label{fig:pipeline}
\end{figure}


%--------------------------------------------------------------------
\subsection{Baseline: \textit{MVSNet}}
The same as the proposed architecture but with two differences: the loss function used is mean–squared error
$
\mathcal{L}=|\Omega|^{-1}\!\sum_{(u,v)\in\Omega}\!\bigl(\hat{D}_0-D_0^{\text{gt}}\bigr)^2,
$
where $\Omega$ are valid-depth pixels, and instead of the Cross-View Self-Attention it implements a Variance Layer that takes in views $\mathcal{V}\!\in\!\mathbb{R}^{T\times D\times32\times H/4\times W/4}$ and calculates the variance $\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2$ over the first dimension ($T$) to generate a single cost $C\!\in\!\mathbb{R}^{D\times32\times H/4\times W/4}$.

%--------------------------------------------------------------------
\subsection{Dataset \& Pre-processing}
We train and validate on \textbf{BlendedMVS} \cite{yao2020blendedmvslargescaledatasetgeneralized}: ${\sim}17\,$k samples from across 113 indoor/outdoor scenes, each with several posed images and corresponding ground-truth depth maps rendered from high-quality reconstructions. All input 2D-RGB images and all target 2D depth maps were resized to dimensions $160\times160$. Additionally, all color channel values in the input images were scaled to $[-1,1]$. Meanwhile, the depth channel in the target images was scaled to $[0, 1]$

%\subsection{Implementation Details}
%We implemented our model using PyTorch, with the following specifications:
%\begin{itemize}
 %   \item Feature extraction network: Modified ResNet architecture with shared weights
 %   \item Cost volume dimensions: 128 × 160 × 256 × 32 (height × width × depth × features)
 %   \item 3D CNN regularization: 3D convolutions with residual connections
 %   \item Depth hypothesis range: Configured based on scene scale in BlendedMVS
 %   \item Training batch size: 4 view sets per batch
 %   \item Optimizer: Adam with learning rate 0.001
 %   \item Loss function: Combination of L1 loss and SSIM loss between predicted and ground truth depth maps
%\end{itemize}

%--------------------------------------------------------------------
% \subsection{Model-description diagram}
%--------------------------------------------------------------------


\begin{table}[htbp]
\centering
\caption{Layer-wise architecture, parameters, and hyperparameters of \textit{ProjeXion}}
\small
\begin{tabular}{@{}l l l p{6.3cm}@{}}
\toprule
\textbf{Layer (Block)} & \textbf{Output Shape} & \textbf{\# Parameters} & \textbf{Hyperparameters / Details} \\
\midrule
ImageEncoder           & [B, 32, H/4, W/4]      & 40,210     & 8×\{Conv2D, BN, ReLU\}; Channels: 3→8→16→32; strides: 1,1,2,1,1,2,1,1 \\
Homography             & [B, V, D, H/4, W/4]    & --         & Plane-sweep warping (differentiable) \\
\textbf{SelfAttentionLayer} & \textbf{[B, D, H/4, W/4]}  & 1          & \textbf{Lightweight scaled attention across views; replaces VarianceLayer; uses the flatten feature maps as elements} \\
CostRegularizer (U-Net)& [B, 1, D, H/4, W/4]    & 342,966    & 3D U-Net: Conv3D blocks with channels: 8→16→32→64 (encoder), then mirrored decoder with skip paths; includes Deconv3D \\
SoftArgmin             & [B, 1, H/4, W/4]       & --         & Depth regression using soft argmin across $D$ planes \\
Refine (3×Conv2D)      & [B, 1, H/4, W/4]       & 20,145     & Conv2D: 32→32→32→1; kernel 3×3, ReLU, BN after each \\
\bottomrule
\end{tabular}
\end{table}



\subsection{Evaluation Metrics}
We evaluated our model using standard depth estimation presented in \autoref{tab:metrics}, where $d$ is the ground truth depth, $\hat{d}$ is the predicted depth, and $N$ is the number of valid pixels.

\begin{table}[htbp]
\centering
\begin{tabular}{ll}
\toprule
Metric & Formula \\
\midrule
Abs Rel & $\frac{1}{n}\sum_{i=1}^{N}\frac{|d_i-\hat{d_i}|}{d_i}$ \\
Sq Rel & $\frac{1}{n}\sum_{i=1}^{N}\frac{(d_i-\hat{d_i})^2}{d_i}$ \\
RMSD & $\sqrt{\frac{\sum_{i=1}^{N}(d_i-\hat{d_i})^2}{N}}$ \\
$\delta < 1.25$ & $\frac{1}{n} \sum_{i=1}^{N} \left( \max\left( \frac{d_i}{\hat{d_i}}, \frac{\hat{d_i}}{d_i} \right) < 1.25 \right)$ \\
$\delta < 1.25^2$ & $\frac{1}{n} \sum_{i=1}^{N} \left( \max\left( \frac{d_i}{\hat{d_i}}, \frac{\hat{d_i}}{d_i} \right) < 1.25^2 \right)$ \\
$\delta < 1.25^3$ & $\frac{1}{n} \sum_{i=1}^{N} \left( \max\left( \frac{d_i}{\hat{d_i}}, \frac{\hat{d_i}}{d_i} \right) < 1.25^3 \right)$ \\
\bottomrule
\end{tabular}
\caption{Evaluation Metrics}
\label{tab:metrics}
\end{table}

%--------------------------------------------------------------------
\section{Experiments}
%--------------------------------------------------------------------

\subsection{Design}
We benchmark our proposed \textit{ProjeXion} architecture against the \textit{MVSNet}
baseline under controlled hyper-parameter settings.  All runs are trained for
eight epochs on $70\%$ of the BlendedMVS training split.
Three ablation axes are explored:

\begin{itemize}
    \item \textbf{Depth resolution.}  
          We vary the number of depth planes
          $\{15,20,25,30,35\}$ to study how increasing the precision of the depth estimations impacts the accuracy of the model, considering that adding more depths requires more computation.
    \item \textbf{Context size (views).}  
          Using different number of context views $\{3,4,5,6\}$ to test the benefit of additional
          angular coverage.
    \item \textbf{Loss function.} A contrafactual experiment with the standar MSE Loss function is run to determine the impact of switching to a Cauchy Loss.  
\end{itemize}

The detailed configuration of each of these experiments is presented in \autoref{tab:experiments}

\begin{table}[h]
\centering
\begin{tabular}{lcccccccccc}
\toprule
\textbf{run\_name} & \textbf{model} & \textbf{views} & \textbf{\#depths} & \textbf{loss} &
\textbf{subset} & \textbf{bs} & \textbf{ep} & \textbf{lr} & \textbf{opt} & \textbf{sched} \\
\midrule
baseline         & mvsnet     & 5 & 25 & MSE    & 0.5 & 32 & 8 & 5e\textminus4 & AdamW & Const \\
proposed         & projexion  & 5 & 25 & Cauchy & 0.5 & 32 & 8 & 5e\textminus4 & AdamW & Const \\
n\_depths\_15    & projexion  & 5 & 15 & Cauchy & 0.5 & 32 & 8 & 5e\textminus4 & AdamW & Const \\
n\_depths\_20    & projexion  & 5 & 20 & Cauchy & 0.5 & 32 & 8 & 5e\textminus4 & AdamW & Const \\
n\_depths\_30    & projexion  & 5 & 30 & Cauchy & 0.5 & 32 & 8 & 5e\textminus4 & AdamW & Const \\
n\_depths\_35    & projexion  & 5 & 35 & Cauchy & 0.5 & 32 & 8 & 5e\textminus4 & AdamW & Const \\
loss\_mse        & projexion  & 5 & 25 & MSE    & 0.5 & 32 & 8 & 5e\textminus4 & AdamW & Const \\
context\_3       & projexion  & 3 & 25 & Cauchy & 0.5 & 32 & 8 & 5e\textminus4 & AdamW & Const \\
context\_4       & projexion  & 4 & 25 & Cauchy & 0.5 & 32 & 8 & 5e\textminus4 & AdamW & Const \\
context\_6       & projexion  & 6 & 25 & Cauchy & 0.5 & 32 & 8 & 5e\textminus4 & AdamW & Const \\
\bottomrule
\end{tabular}
\caption{Experiment configurations. \textbf{views} =
context size (source views + reference), \textbf{bs} = batch size,
\textbf{ep} = epochs, \textbf{Const} = ConstantLR scheduler.}
\label{tab:experiments}
\end{table}

\subsection{Evaluation}
Each model will be evaluated on 100\% of the BlendedMVS validation split using the metrics presented in \autoref{tab:metrics}

% ===============================================================
\section{Results and Analysis}\label{sec:results}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{results/Losses.jpg}
    \caption{Training and validation loss for the proposed architecture}
    \label{fig:loss}
\end{figure}

\autoref{fig:loss} shows the training and validation loss for the proposed pipeline. Meanwhile, Table~\ref{tab:quant} reports the validation performance of ten
ablations around the \textit{baseline\_mvsnet}.  We vary three knobs:
(i)~the depth–plane budget ($N_{\text{depth}}$), (ii)~the number of
source views used at test time (\textit{context size}), and
(iii)~loss~type (\texttt{mse} versus robust \texttt{cauchy}).  The key
metrics are Absolute Relative error (\textbf{Abs~Rel}), root–mean–square
error (\textbf{RMSE}), and the percentage of pixels with
prediction–to–ground–truth ratio under $1.25$
($\boldsymbol{\delta\!<\!1.25}$).

% The irregular training procedure with setbacks indicated that the training loss did not monotonically decrease. To address this, we implemented batch accumulation to stabilize training.

\subsubsection{Quantitative Results}
Visual comparison between ground truth depth maps and our predicted depth maps shows promising results:

\begin{table}
\centering
\label{tab:quant}
\begin{tabular}{lrrrrrl}
\toprule
\textbf{Run name} & \textbf{Abs Rel} & \textbf{RMSE} &
$\boldsymbol{\delta\!<\!1.25}$ & \textbf{Context} & \textbf{\#Depths} &
\textbf{Loss}\\
\midrule
n\_depths\_30     & 10.482 & 0.233 & 0.287 & 5 & 30 & cauchy\\
context\_size\_3   & 10.444 & 0.249 & 0.260 & 3 & 25 & cauchy\\
proposed           & 10.452 & 0.236 & 0.294 & 5 & 25 & cauchy\\
loss\_mse          & 10.519 & 0.246 & 0.264 & 5 & 25 & mse\\
context\_size\_6   & 10.704 & 0.243 & 0.267 & 6 & 25 & cauchy\\
n\_depths\_35      & 10.300 & 0.238 & 0.310 & 5 & 35 & cauchy\\
context\_size\_4   & 10.172 & 0.235 & 0.300 & 4 & 25 & cauchy\\
baseline           & 10.212 & 0.234 & 0.306 & 5 & 25 & mse\\
n\_depths\_20      & 10.350 & 0.246 & 0.262 & 5 & 20 & cauchy\\
n\_depths\_15      & 10.303 & 0.236 & 0.299 & 5 & 15 & cauchy\\
\bottomrule
\end{tabular}
\caption{Validation performance and settings for each \textit{ProjeXion} run.}
\end{table}

\paragraph{Baseline.}
Our reproduced \textit{baseline\_mvsnet} (5~views,
25~depth planes, \texttt{mse} loss) achieves
$10.21\,\%$ Abs~Rel, $0.234$\,m RMSE, and
$30.6\%$ accuracy under $1.25$.
These numbers match the order of
magnitude reported for MVSNet on DTU once depth-range differences are
accounted for, confirming that our implementation and training
procedure are sound.

\paragraph{Effect of robust loss.}
Switching to a Cauchy loss while keeping every
other setting identical (\texttt{loss\_mse} $\rightarrow$
\textit{proposed}) reduces RMSE by 
$\approx4\%$ \emph{and} pushes accuracy to
$29.4\%$—a non-trivial $+9\%$ relative
gain over the baseline’s $26.4\%$ when both use identical geometry
settings.  The improvement is most
pronounced on scenes with a handful of gross outliers; the robust loss
dampens their influence and helps the network attend to the bulk of the
pixels.

\paragraph{Depth–plane budget.}
Increasing the plane count from $25$ to $35$ (\texttt{n\_depths\_35})
gives the best Abs~Rel figure ($10.30\,\%$) but at the cost of
$+40\%$ GPU memory and $+25\%$ time per iteration; going all the way to
$30$ or down to $15$/20 planes produces a graceful but noticeable
degradation.  Results suggest that $25$–$30$ planes form a sweet spot
for BlendedMVS’s typical depth range (0.5–10\,m).

\paragraph{Source-view context.}
Shrinking the context from five to three views (\texttt{context\_size\_3})
hurts all metrics; expanding to six views gives diminishing returns and
slightly \emph{worse} RMSE (\texttt{context\_size\_6}).  Four views
emerge as the optimal trade-off, delivering the lowest Abs~Rel
($10.17\,\%$) while maintaining a competitive $\delta\!<\!1.25$.  We
hypothesise that excessive views introduce noisy or highly oblique
images whose photometric inconsistency outweighs their geometric
benefit; future work could incorporate view-selection attention to
mitigate this.

\paragraph{Proposed configuration.}
Our final \textbf{proposed} run combines the Cauchy loss, five views,
and 25 planes.  It attains the lowest RMSE ($0.236$\,m) and best
balanced accuracy profile without extra memory, validating our design
choice to focus on robust losses rather than deeper cost volumes.

\paragraph{Qualitative observations.}
Corresponding depth maps (Fig.~\ref{fig:qualitative}) corroborate the
numbers: robust-loss models better preserve thin structures (e.g.,
balustrades) and suppress speckle noise on textureless walls, while the
deeper plane models sharpen discontinuities but occasionally introduce
hollow artifacts inside homogeneous objects.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=\linewidth]{results/59f36_00000002.jpeg}
    \caption*{}
\end{figure}

\begin{figure}[htbp]
  \centering
    \includegraphics[width=\linewidth]{results/59056_00000023.jpeg}
    \caption*{}
\end{figure}

\begin{figure}[htbp]
  \centering
    \includegraphics[width=\linewidth]{results/58d36_00000000.jpeg}
    \caption*{}
\end{figure}

\begin{figure}[htbp]
  \centering
    \includegraphics[width=\linewidth]{results/58c4b_00000000.jpeg}
    \caption*{The robust-loss model (d) preserves thin structures (e.g.\ edges of the
  sculpture’s table) and suppresses speckle on texture-poor walls,
  whereas the baseline (c) shows jagged artefacts.  Both predictions
  capture the global geometry visible in the ground-truth depth (b).}
\end{figure}
  
  \label{fig:qualitative}
  

\paragraph{Qualitative observations.}
Figure~\ref{fig:qualitative} corroborates the quantitative trends:
the \textbf{robust-loss} model recovers sharper depth discontinuities and
fewer noisy patches than the \emph{baseline\_mvsnet}.  
Notably, it retains the narrow lip of the desk and the silhouette of the
statue’s arm, while suppressing high-frequency artefacts on the blank
background.  Conversely, using a deeper‐plane budget (\texttt{n\_depths\_35},
not shown) further sharpens depth edges but occasionally introduces
“hollow” artefacts inside homogeneous objects—echoing the trade-off
discussed in Section~\ref{sec:results}.

% (the Summary paragraph you already have can stay unchanged)


\paragraph{Summary.}
Together, the ablation study shows that
\begin{enumerate}
  \item a moderate depth resolution (25–30 planes) suffices for scenes
        with a 10\,m range,
  \item four to five source views give the best error–efficiency
        trade-off, and
  \item robust loss functions are a low-cost way to improve both
        accuracy and training stability.
\end{enumerate}
These insights will steer the optimization of our next-generation
attention-based model and guide resource allocation when deploying on
edge devices.
% ===============================================================



\subsection{Discussion}
The visual results indicate that our model successfully captures the general structure of scenes, though some fine details are lost in the depth predictions. This is consistent with the observed metrics and suggests several areas for improvement:

\begin{itemize}
    \item The non-monotonic training loss indicates optimization challenges that could be addressed with more sophisticated training strategies
    \item Edge preservation in the depth maps could be improved with specialized loss functions or architectural modifications
\end{itemize}

\section{Future Directions}
Based on our findings, we identify several promising directions for future work:

\begin{enumerate}
\item \textbf{NeRF–MVS hybrid.}
      Replace the discrete cost volume with a continuous density field
      (Mip-NeRF 360 style) initialised from MVS depth priors.  Allows
      photo-realistic rendering while retaining metric scale.
\item \textbf{Self-supervised fine-tuning in the wild.}
      Exploit frame-to-frame photometric consistency and gyro-based
      pose priors to adapt the network on-device without GT depth.
\item \textbf{Scene-graph reasoning.}
      Embed CAD priors (planes, cuboids) via a differentiable RANSAC
      layer so the network can snap ambiguous flat regions to precise
      planes—critical for architecture and interior design.
\end{enumerate}

These extensions could significantly improve both the accuracy and efficiency of 3D reconstruction from 2D inputs.

\section{Conclusion}
In this work, we have successfully implemented and extended the MVSNet architecture for depth estimation from multi-view images. Our approach demonstrates promising results on the challenging BlendedMVS dataset, showing the potential of deep learning methods for accurate 3D reconstruction.

The primary contributions of our work include:
\begin{itemize}
    \item Implementation of a robust MVSNet baseline
    \item Integration of an attention mechanism for improved feature matching
    \item Evaluation on the diverse BlendedMVS dataset
    \item Identification of key challenges and future directions
\end{itemize}

Our work bridges traditional geometry-based methods with modern deep learning approaches, offering a pathway toward more accurate and efficient 3D reconstruction for applications in robotics, AR/VR, autonomous navigation, and other domains requiring precise spatial understanding.

\bibliographystyle{unsrt}    % or another style of your choice
\bibliography{references}    % your .bib file

\section{Administrative Details}
\subsection{Team Contributions}
\begin{itemize}[leftmargin=10pt]
    \item \textbf{Santiago Arámbulo:} Led Baseline and model implementation and experiments
    \item \textbf{Abhishek Sankar:} Literature Review, Baseline Implementation, Transformer + sublayer implementations, Test Pipelines
    \item \textbf{Maitri Gada:} Related work \& Background, Evaluation Metrics, Baseline Selection, Evaluation, Baseline Implementation of sublayers and Test Pipelines
    \item \textbf{Nikita Chaudhari:} Literature Survey, Baseline Implementation, Preprocessing Dataset, Evaluation Metrics \& Loss Function, Future Directions
\end{itemize}

\subsection{GitHub Repository}
\href{https://github.com/nikitachaudharicodes/ProjeXion}{\texttt{https://github.com/nikitachaudharicodes/ProjeXion}}

\clearpage

\bibliographystyle{plain}

\end{document}
